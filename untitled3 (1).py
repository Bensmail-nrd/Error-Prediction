# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vG-3Z9uIxyt7H37Xb8YVgik9oo04gE-1

# Test methods
---


## Utils
"""


import numpy as np
import pandas as pd
import json
import pickle
import time as ttime
import os
import threading
import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
import keras.callbacks
from keras.models import Sequential
from keras.layers import *
from keras.callbacks import CSVLogger,EarlyStopping, ModelCheckpoint
from keras.metrics import RootMeanSquaredError
from keras.optimizers import Adam
from keras.models import load_model
from sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error,hamming_loss,mean_absolute_percentage_error
import time as ttime
from sklearn.svm import SVR
from sklearn.ensemble import RandomForestRegressor
import random
from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LinearRegression
import warnings
warnings.filterwarnings('ignore')

df = pd.read_csv('./PEMSD8.dyna')
res = df.loc[df['entity_id']==154]
res.describe().to_json('./D8_154.json')

def split(x_train,y_train):
    """
    This function returns x_train,y_train,x_test,y_test,x_val,y_val
    """
    train = int(len(x_train)* 0.7)
    test = len(x_train) - train
    val = int(test / 3)
    test = test - val
    return x_train[:train],y_train[:train],x_train[train:train + test],y_train[train:train + test],x_train[-val:],y_train[-val:]

def to_supervised(df:pd.DataFrame,window = 14,index =  0):
  x,y = [],[]
  for i in range(len(df) - window ):
    x.append(df.iloc[i:i+window])
    y.append(df.iloc[i+window,index])
  return np.array(x),np.array(y)

def mape(y_test,y_pred):
    c = []
    if len(y_test.shape)==1:
        for i in range(len(y_test)):
            if y_test[i]!=0:
                c.append(abs((y_test[i]-y_pred[i])/y_test[i]))
        return  np.mean(c) * 100
    else:
        d= []
        for i in range(len(y_test[0])):
            d.append(mape(y_test[:,i],y_pred[:,i]))
        return np.mean(d)

def train_model(model_name:str,x_train,y_train,x_test,y_test,i=1,window=14,work_dir='.',epochs=100,batch_size=16,verbose=0):
    early = EarlyStopping(monitor='val_loss',patience=8)
    filepath = work_dir + f'/{model_name}.h5'
    check = ModelCheckpoint(filepath=filepath,save_best_only=True,monitor='val_loss')
    if model_name =='LSTM':
        m = Sequential() 
        m.add((LSTM(64, return_sequences=True, input_shape=(window,3)))) 
        # m.add(BatchNormalization())
        # m.add(Dropout(0.2)) 
        m.add((LSTM(64))) 
        # m.add(BatchNormalization())
        m.add(Dense(i)) 
        m.compile(optimizer=Adam(),loss="mae")
        start = ttime.time()
        m.fit(x_train,y_train,epochs=epochs,verbose=verbose,validation_split=0.2,callbacks=[early,check],batch_size=batch_size)
        end = ttime.time()
        TIME = end - start
        m = load_model(filepath)
        y_pred = m.predict(x_test)
        return y_pred,TIME
    if model_name =='Bi-LSTM':
        m = Sequential()
        m.add(Bidirectional(LSTM(64, return_sequences=True), input_shape=(window,3)))
        m.add(Bidirectional(LSTM(64, return_sequences=True)))
        m.add(Bidirectional(LSTM(64, return_sequences=True)))
        m.add(Bidirectional(LSTM(64, return_sequences=True)))
        m.add(Bidirectional(LSTM(64)))
        m.add(Dense(i))
        m.compile(optimizer=Adam(),loss="mae")
        start = ttime.time()
        m.fit(x_train,y_train,epochs=epochs,verbose=verbose,validation_split=0.2,callbacks=[early,check],batch_size=batch_size)
        end = ttime.time()
        TIME = end - start
        m = load_model(filepath)
        y_pred = m.predict(x_test)
        return y_pred,TIME
    if model_name =='CNN':
        m = Sequential() 
        m.add(Conv1D(128, (3), input_shape=(window,3)))
        m.add(MaxPooling1D( 2))
        m.add(Flatten())
        m.add(Dense(128))
        m.add(Dense(1))
        m.compile(optimizer=Adam(),loss="mae")
        start = ttime.time()
        m.fit(x_train,y_train,epochs=epochs,verbose=verbose,validation_split=0.2,callbacks=[early,check],batch_size=batch_size)
        end = ttime.time()
        TIME = end - start
        m = load_model(filepath)
        y_pred = m.predict(x_test)
        return y_pred,TIME
    elif model_name == 'GRU':
        m = Sequential() 
        m.add((GRU(128, return_sequences=True, input_shape=(window,3)))) 
        # m.add(BatchNormalization())
        # m.add(Dropout(0.2)) 
        # m.add((GRU(25, activation='relu', return_sequences=True))) 
        # m.add(BatchNormalization())
        # m.add(Dropout(0.2)) 
        m.add((GRU(64))) 
        # m.add(BatchNormalization())
        # m.add(Dropout(0.2)) 
        # m.add(BatchNormalization())
        m.add(Dense(i)) 
        m.compile(optimizer=Adam(),loss="mae")
        start = ttime.time()
        m.fit(x_train,y_train,epochs=epochs,verbose=verbose,validation_split=0.2,callbacks=[early,check],batch_size=batch_size)
        end = ttime.time()
        TIME = end - start
        m = load_model(filepath)
        y_pred = m.predict(x_test)
        return y_pred,TIME
    elif model_name =='CNN LSTM':
        m = Sequential() 
        m.add(Conv1D(64, (3), input_shape=(window,3)))
        # m.add(MaxPooling1D( 2))
        m.add((LSTM(64,return_sequences=True))) 
        m.add((LSTM(64))) 
        m.add((Flatten())) 
        m.add(Dense(1)) 
        m.compile(optimizer=Adam(),loss="mae")
        start = ttime.time()
        m.fit(x_train,y_train,epochs=epochs,verbose=verbose,validation_split=0.2,callbacks=[early,check],batch_size=batch_size)
        end = ttime.time()
        TIME = end - start
        m = load_model(filepath)
        y_pred = m.predict(x_test)
        return y_pred,TIME
    elif model_name =='SVR':
        x = x_train[:,:,0]
        xt = x_test[:,:,0]
        svr= SVR()
        start = ttime.time()
        svr.fit(x,y_train)
        end = ttime.time()
        TIME = end - start
        y_pred = svr.predict(xt)
        return y_pred,TIME
    elif model_name =='KNN':
        x = x_train[:,:,0]
        xt = x_test[:,:,0]
        k = 3  # Number of neighbors to consider
        knn = KNeighborsRegressor(n_neighbors=k)
        start = ttime.time()
        knn.fit(x, y_train)
        end = ttime.time()
        TIME = end - start
        y_pred = knn.predict(xt)
        print(mean_absolute_error(y_test,y_pred))
        return y_pred,TIME
    elif model_name =='LR':
        x = x_train[:,:,0]
        xt = x_test[:,:,0]
        model = LinearRegression()
        start = ttime.time()
        model.fit(x, y_train)
        end = ttime.time()
        TIME = end - start
        y_pred = model.predict(xt)
        return y_pred,TIME

def better(results:list[pd.DataFrame]):
    last = []
    for res in results:
        if all(res['%'] > 0 ):
            last.append(res)

    if last.__len__() !=0:
        max_value = sum(last[0]['%'])
        max_df = (last[0])
        for res in last:
            if max_value < sum(res['%']):
                max_df = res
                max_value = sum(res['%'])
        return max_df
    else:
        max_value = sum(results[0]['%'])
        max_df = (results[0])
        for res in results:
            if max_value < sum(res['%']):
                max_df = res
                max_value = sum(res['%'])
        return max_df

def runTests(df,name:str,epochs,mean_iter=5,min_win=2,max_win=50,verbose=0,batch_size=8,val_split=0.2,patience=8):
    results = []
    times = []
    real,prediction = df.iloc[:,0].to_numpy().reshape(-1,),df.iloc[:,1].to_numpy().reshape(-1,)
    error = real - prediction
    length = len(error)
    test_length = int(length * 0.3)
    for window in [2,3,4,5,6,7,8,9,10]:
        x,y = [],[]
        for i in range(len(error) - window):
            x.append(error[i:i+window])
            y.append(error[i+window]) 
        x,y = np.array(x),np.array(y)
        x_train,y_train = x[:test_length],y[:test_length]
        for k in [2,3]:
            knn = KNeighborsRegressor(n_neighbors=k)
            start = ttime.time()
            knn.fit(x_train, y_train)
            end = ttime.time()
            TIME = end - start
            error_prediction = knn.predict(x[test_length:])
            test = pd.DataFrame(data={'r':real[test_length + window:],'p':prediction[test_length + window:],'o':prediction[test_length + window:] + error_prediction.reshape(-1,)})
            test.to_csv(f'./master_results{TIME}.csv',index=False)
            err = mean_absolute_error(real[test_length + window:],prediction[test_length + window:] + error_prediction.reshape(-1,))
            mse_err = mean_squared_error(real[test_length + window:],prediction[test_length + window:] + error_prediction.reshape(-1,))
            mape_err = mape(real[test_length + window:],prediction[test_length + window:] + error_prediction.reshape(-1,))
            result3 = pd.DataFrame(data={'Metrics':['MAE','MSE','MAPE'],'Normal values':[float(mean_absolute_error(real[test_length + window:],prediction[test_length + window:])),float(mean_squared_error(real[test_length + window:],prediction[test_length + window:])),float(mape(real[test_length + window:],prediction[test_length + window:]))],'With Optimization':[float(err),float(mse_err),mape_err],'Profits':[float(mean_absolute_error(real[test_length + window:],prediction[test_length + window:]) - err),float(mean_squared_error(real[test_length + window:],prediction[test_length + window:]) - mse_err),(mape(real[test_length + window:],prediction[test_length + window:]) - mape_err)],'%':[float((mean_absolute_error(real[test_length + window:],prediction[test_length + window:]) - err)/mean_absolute_error(real[test_length + window:],prediction[test_length + window:])),float((mean_squared_error(real[test_length + window:],prediction[test_length + window:]) - mse_err)/mean_squared_error(real[test_length + window:],prediction[test_length + window:])),float((mape(real[test_length + window:],prediction[test_length + window:]) - mape_err)/(mape(real[test_length + window:],prediction[test_length + window:])))]})
            results.append(result3)
            times.append(TIME) 
            if all(result3['%']>0):
                break
    best_df = better(results)
    if any(best_df['%'] < 0):
        window = 12
        early = EarlyStopping(monitor='val_loss',patience=8)
        best = ModelCheckpoint(filepath='./best.h5',save_best_only=True,monitor='val_loss')
        x,y = [],[]
        for i in range(len(error) - window):
            x.append(error[i:i+window])
            y.append(error[i+window]) 
        x,y = np.array(x),np.array(y)
        x_train,y_train = x[:test_length],y[:test_length]
        model = Sequential()
        model.add(Bidirectional(LSTM(64, return_sequences=True), input_shape=(window,1))) 
        model.add(Bidirectional(LSTM(64)))
        model.add(Dense(1))
        model.compile(optimizer=Adam(),loss="mae",metrics=[RootMeanSquaredError()])
        start = ttime.time()
        history = model.fit(x_train,y_train,epochs=epochs,callbacks=[early,best],validation_split=val_split,verbose=verbose,batch_size=batch_size)
        end = ttime.time()
        TIME = end - start
        model = load_model('./best.h5')
        error_prediction = model.predict(x[test_length:])
        err = mean_absolute_error(real[test_length + window:],prediction[test_length + window:] + error_prediction.reshape(-1,))
        mse_err = mean_squared_error(real[test_length + window:],prediction[test_length + window:] + error_prediction.reshape(-1,))
        mape_err = mape(real[test_length + window:],prediction[test_length + window:] + error_prediction.reshape(-1,))
        result1 = pd.DataFrame(data={'Metrics':['MAE','MSE','MAPE'],'Normal values':[float(mean_absolute_error(real[test_length + window:],prediction[test_length + window:])),float(mean_squared_error(real[test_length + window:],prediction[test_length + window:])),float(mape(real[test_length + window:],prediction[test_length + window:]))],'With Optimization':[float(err),float(mse_err),mape_err],'Profits':[float(mean_absolute_error(real[test_length + window:],prediction[test_length + window:]) - err),float(mean_squared_error(real[test_length + window:],prediction[test_length + window:]) - mse_err),(mape(real[test_length + window:],prediction[test_length + window:]) - mape_err)],'%':[float((mean_absolute_error(real[test_length + window:],prediction[test_length + window:]) - err)/mean_absolute_error(real[test_length + window:],prediction[test_length + window:])),float((mean_squared_error(real[test_length + window:],prediction[test_length + window:]) - mse_err)/mean_squared_error(real[test_length + window:],prediction[test_length + window:])),float((mape(real[test_length + window:],prediction[test_length + window:]) - mape_err)/(mape(real[test_length + window:],prediction[test_length + window:])))]})
        results.append(result1)
        times.append(TIME)
        if any(result1['%'] < 0):
            window = 4
            early = EarlyStopping(monitor='val_loss',patience=8)
            best = ModelCheckpoint(filepath='./best.h5',save_best_only=True,monitor='val_loss')
            x,y = [],[]
            for i in range(len(error) - window):
                x.append(error[i:i+window])
                y.append(error[i+window]) 
            x,y = np.array(x),np.array(y)
            x_train,y_train = x[:test_length],y[:test_length]
            model = Sequential()
            model.add(Bidirectional(LSTM(64, return_sequences=True), input_shape=(window,1))) 
            model.add(Bidirectional(LSTM(64)))
            model.add(Dense(1))
            model.compile(optimizer=Adam(),loss="mae",metrics=[RootMeanSquaredError()])
            start = ttime.time()
            history = model.fit(x_train,y_train,epochs=epochs,callbacks=[early,best],validation_split=val_split,verbose=verbose,batch_size=batch_size)
            end = ttime.time()
            TIME = end - start
            model = load_model('./best.h5')
            error_prediction = model.predict(x[test_length:])
            err = mean_absolute_error(real[test_length + window:],prediction[test_length + window:] + error_prediction.reshape(-1,))
            mse_err = mean_squared_error(real[test_length + window:],prediction[test_length + window:] + error_prediction.reshape(-1,))
            mape_err = mape(real[test_length + window:],prediction[test_length + window:] + error_prediction.reshape(-1,))
            result1 = pd.DataFrame(data={'Metrics':['MAE','MSE','MAPE'],'Normal values':[float(mean_absolute_error(real[test_length + window:],prediction[test_length + window:])),float(mean_squared_error(real[test_length + window:],prediction[test_length + window:])),float(mape(real[test_length + window:],prediction[test_length + window:]))],'With Optimization':[float(err),float(mse_err),mape_err],'Profits':[float(mean_absolute_error(real[test_length + window:],prediction[test_length + window:]) - err),float(mean_squared_error(real[test_length + window:],prediction[test_length + window:]) - mse_err),(mape(real[test_length + window:],prediction[test_length + window:]) - mape_err)],'%':[float((mean_absolute_error(real[test_length + window:],prediction[test_length + window:]) - err)/mean_absolute_error(real[test_length + window:],prediction[test_length + window:])),float((mean_squared_error(real[test_length + window:],prediction[test_length + window:]) - mse_err)/mean_squared_error(real[test_length + window:],prediction[test_length + window:])),float((mape(real[test_length + window:],prediction[test_length + window:]) - mape_err)/(mape(real[test_length + window:],prediction[test_length + window:])))]})
            results.append(result1)
            times.append(TIME)
            if any(result1['%'] < 0):

                early = EarlyStopping(monitor='val_loss',patience=8)
                best = ModelCheckpoint(filepath='./best.h5',save_best_only=True,monitor='val_loss')
                window = 14
                x,y = [],[]
                for i in range(len(error) - window):
                    x.append(error[i:i+window])
                    y.append(error[i+window]) 
                x,y = np.array(x),np.array(y)
                x_train,y_train = x[:test_length],y[:test_length]
                model = Sequential()
                model.add((LSTM(25, return_sequences=True, input_shape=(window,1)))) 
                model.add(BatchNormalization())
                model.add(Dropout(0.2)) 
                model.add((LSTM(40,return_sequences=True))) 
                model.add(BatchNormalization())
                model.add(Dropout(0.2)) 
                model.add((LSTM(20, return_sequences=True))) 
                model.add((LSTM(5))) 
                model.add(Dense(1))  
                model.compile(optimizer=Adam(),loss="mae",metrics=[RootMeanSquaredError()])
                start = ttime.time()
                history = model.fit(x_train,y_train,epochs=epochs,callbacks=[early,best],validation_split=val_split,verbose=verbose,batch_size=batch_size)
                end = ttime.time()
                TIME = end - start
                model = load_model('./best.h5')
                error_prediction = model.predict(x[test_length:])
                err = mean_absolute_error(real[test_length + window:],prediction[test_length + window:] + error_prediction.reshape(-1,))
                mse_err = mean_squared_error(real[test_length + window:],prediction[test_length + window:] + error_prediction.reshape(-1,))
                mape_err = mape(real[test_length + window:],prediction[test_length + window:] + error_prediction.reshape(-1,))
                result2 = pd.DataFrame(data={'Metrics':['MAE','MSE','MAPE'],'Normal values':[float(mean_absolute_error(real[test_length + window:],prediction[test_length + window:])),float(mean_squared_error(real[test_length + window:],prediction[test_length + window:])),float(mape(real[test_length + window:],prediction[test_length + window:]))],'With Optimization':[float(err),float(mse_err),mape_err],'Profits':[float(mean_absolute_error(real[test_length + window:],prediction[test_length + window:]) - err),float(mean_squared_error(real[test_length + window:],prediction[test_length + window:]) - mse_err),(mape(real[test_length + window:],prediction[test_length + window:]) - mape_err)],'%':[float((mean_absolute_error(real[test_length + window:],prediction[test_length + window:]) - err)/mean_absolute_error(real[test_length + window:],prediction[test_length + window:])),float((mean_squared_error(real[test_length + window:],prediction[test_length + window:]) - mse_err)/mean_squared_error(real[test_length + window:],prediction[test_length + window:])),float((mape(real[test_length + window:],prediction[test_length + window:]) - mape_err)/(mape(real[test_length + window:],prediction[test_length + window:])))]})
                results.append(result2)
                times.append(TIME)
                if any(result2['%'] < 0):
                    early = EarlyStopping(monitor='val_loss',patience=8)
                    best = ModelCheckpoint(filepath='./best.h5',save_best_only=True,monitor='val_loss')
                    window = 4
                    x,y = [],[]
                    for i in range(len(error) - window):
                        x.append(error[i:i+window])
                        y.append(error[i+window]) 
                    x,y = np.array(x),np.array(y)
                    x_train,y_train = x[:test_length],y[:test_length]
                    model = Sequential()
                    model.add((LSTM(25, return_sequences=True, input_shape=(window,1)))) 
                    model.add(BatchNormalization())
                    model.add(Dropout(0.2)) 
                    model.add((LSTM(40,return_sequences=True))) 
                    model.add(BatchNormalization())
                    model.add(Dropout(0.2)) 
                    model.add((LSTM(20, return_sequences=True))) 
                    model.add((LSTM(5))) 
                    model.add(Dense(1))  
                    model.compile(optimizer=Adam(),loss="mae",metrics=[RootMeanSquaredError()])
                    start = ttime.time()
                    history = model.fit(x_train,y_train,epochs=epochs,callbacks=[early,best],validation_split=val_split,verbose=verbose,batch_size=batch_size)
                    end = ttime.time()
                    TIME = end - start
                    model = load_model('./best.h5')
                    error_prediction = model.predict(x[test_length:])
                    err = mean_absolute_error(real[test_length + window:],prediction[test_length + window:] + error_prediction.reshape(-1,))
                    mse_err = mean_squared_error(real[test_length + window:],prediction[test_length + window:] + error_prediction.reshape(-1,))
                    mape_err = mape(real[test_length + window:],prediction[test_length + window:] + error_prediction.reshape(-1,))
                    result2 = pd.DataFrame(data={'Metrics':['MAE','MSE','MAPE'],'Normal values':[float(mean_absolute_error(real[test_length + window:],prediction[test_length + window:])),float(mean_squared_error(real[test_length + window:],prediction[test_length + window:])),float(mape(real[test_length + window:],prediction[test_length + window:]))],'With Optimization':[float(err),float(mse_err),mape_err],'Profits':[float(mean_absolute_error(real[test_length + window:],prediction[test_length + window:]) - err),float(mean_squared_error(real[test_length + window:],prediction[test_length + window:]) - mse_err),(mape(real[test_length + window:],prediction[test_length + window:]) - mape_err)],'%':[float((mean_absolute_error(real[test_length + window:],prediction[test_length + window:]) - err)/mean_absolute_error(real[test_length + window:],prediction[test_length + window:])),float((mean_squared_error(real[test_length + window:],prediction[test_length + window:]) - mse_err)/mean_squared_error(real[test_length + window:],prediction[test_length + window:])),float((mape(real[test_length + window:],prediction[test_length + window:]) - mape_err)/(mape(real[test_length + window:],prediction[test_length + window:])))]})
                    results.append(result2)
                    times.append(TIME)
                    if any(result2['%'] < 0):
                        early = EarlyStopping(monitor='val_loss',patience=8)
                        best = ModelCheckpoint(filepath='./best.h5',save_best_only=True,monitor='val_loss')
                        window = 4
                        x,y = [],[]
                        for i in range(len(error) - window):
                            x.append(error[i:i+window])
                            y.append(error[i+window]) 
                        x,y = np.array(x),np.array(y)
                        x_train,y_train = x[:test_length],y[:test_length]
                        model = Sequential()
                        model.add((LSTM(128, return_sequences=True, input_shape=(window,1)))) 
                        model.add((LSTM(128, return_sequences=True))) 
                        model.add((LSTM(128, return_sequences=True))) 
                        model.add((LSTM(128, return_sequences=True))) 
                        model.add((LSTM(128, return_sequences=True))) 
                        model.add((LSTM(128))) 
                        model.add(Dense(1))  
                        model.compile(optimizer=Adam(),loss="mae",metrics=[RootMeanSquaredError()])
                        start = ttime.time()
                        history = model.fit(x_train,y_train,epochs=epochs,callbacks=[early,best],validation_split=val_split,verbose=verbose,batch_size=batch_size)
                        end = ttime.time()
                        TIME = end - start
                        model = load_model('./best.h5')
                        error_prediction = model.predict(x[test_length:])
                        err = mean_absolute_error(real[test_length + window:],prediction[test_length + window:] + error_prediction.reshape(-1,))
                        mse_err = mean_squared_error(real[test_length + window:],prediction[test_length + window:] + error_prediction.reshape(-1,))
                        mape_err = mape(real[test_length + window:],prediction[test_length + window:] + error_prediction.reshape(-1,))
                        result2 = pd.DataFrame(data={'Metrics':['MAE','MSE','MAPE'],'Normal values':[float(mean_absolute_error(real[test_length + window:],prediction[test_length + window:])),float(mean_squared_error(real[test_length + window:],prediction[test_length + window:])),float(mape(real[test_length + window:],prediction[test_length + window:]))],'With Optimization':[float(err),float(mse_err),mape_err],'Profits':[float(mean_absolute_error(real[test_length + window:],prediction[test_length + window:]) - err),float(mean_squared_error(real[test_length + window:],prediction[test_length + window:]) - mse_err),(mape(real[test_length + window:],prediction[test_length + window:]) - mape_err)],'%':[float((mean_absolute_error(real[test_length + window:],prediction[test_length + window:]) - err)/mean_absolute_error(real[test_length + window:],prediction[test_length + window:])),float((mean_squared_error(real[test_length + window:],prediction[test_length + window:]) - mse_err)/mean_squared_error(real[test_length + window:],prediction[test_length + window:])),float((mape(real[test_length + window:],prediction[test_length + window:]) - mape_err)/(mape(real[test_length + window:],prediction[test_length + window:])))]})
                        results.append(result2)
                        times.append(TIME)

    best_df = better(results)
    # print(best_df)
    index = 0
    for i in results:
        if all(i != best_df):
            index+=1
            break
        
        
    return better(results),times[index]

def get_statistics_for_dataset(df:pd.DataFrame,custom_segments:list,real_test=True,verbose=0,bootsrap_iterations = 5,studies_column='traffic_speed',work_dir:str='',thresh=0.01,epochs=150):
    print('loaded')
    try:
        df['hours'] = pd.to_datetime(df['time']).dt.hour
        df['day'] = pd.to_datetime(df['time']).dt.day_of_week
        df.drop(['time'],inplace=True,axis=1)
    except:
        pass
    studied = df[['entity_id',studies_column,'day','hours']].copy()
    if custom_segments.__len__()==0:
        entity_ids = pd.DataFrame(df.groupby('entity_id').mean()).index
    else: 
        entity_ids = custom_segments
    print('start segments')
    done = True
    for segment_id in entity_ids:
        print('for segment : ',segment_id)
        done = True
        # results_map = {}
        selected_id = studied.loc[studied['entity_id'] == segment_id]
        selected_id.drop(['entity_id'],inplace=True,axis=1)
        x,y = to_supervised(selected_id)
        x_train,y_train,x_test,y_test,x_val,y_val = split(x,y)
        for model in models:
            if not done:
                break
            mae_main = []
            mae_aux = []
            mae_prof = []
            mse_main = []
            mse_aux = []
            mse_prof = []
            mape_main = []
            mape_aux = []
            mape_prof = []
            time_main = []
            time_aux = []
            time_prof = []
            search_time_list = []
            for _ in range(bootsrap_iterations):
                bootstrap_indices = np.random.choice(len(x_train), size=len(x_train), replace=True)
                X_train_bootstrap = x_train[bootstrap_indices]
                y_train_bootstrap = y_train[bootstrap_indices]
                print('train model : ',model)
                y_pred,TIME = train_model(model_name=model,verbose=verbose,batch_size=256,x_train=X_train_bootstrap,x_test=x_test,y_train=y_train_bootstrap,y_test=y_test,epochs=epochs,work_dir=work_dir)
                time_main.append(TIME)
                print('run tests for model ',model)
                ss=pd.DataFrame({'real':y_test.reshape(-1,),'pred':y_pred.reshape(-1,)})
                # ss.to_csv(work_dir + f'/prediction_{model}_{segment_id}_{int(1000 * random.random())}.csv',index=False)
                start = ttime.time()
                results, TIME_2 = runTests(df=ss,name='hh',verbose=0,epochs=400 if real_test else 1)
                end = ttime.time()
                search_time = end - start
                time_aux.append(TIME_2)
                print(results)
                # print(f'MAE:{results.iloc[0,4]:2f},MSE:{results.iloc[1,4]:2f},MAPE:{results.iloc[2,4]:2f}')
                if any(results['%']<0):
                    done = False
                    break

                mae_main.append(results.iloc[0,1])
                mae_aux.append(results.iloc[0,2])
                mae_prof.append(results.iloc[0,4])
                mse_main.append(results.iloc[1,1])
                mse_aux.append(results.iloc[1,2])
                mse_prof.append(results.iloc[1,4])
                mape_main.append(results.iloc[2,1])
                mape_aux.append(results.iloc[2,2])
                mape_prof.append(results.iloc[2,4])
                time_prof.append(TIME_2/TIME)
                search_time_list.append(search_time)

                # print('selected the best')
                # print(results)
            if not done:
                break  
            mae_main_int = np.percentile(mae_main,[2.5,97.5]).tolist()
            mae_aux_int = np.percentile(mae_aux,[2.5,97.5]).tolist()
            mae_prof_int = np.percentile(mae_prof,[2.5,97.5]).tolist()
            mse_main_int = np.percentile(mse_main,[2.5,97.5]).tolist()
            mse_aux_int = np.percentile(mse_aux,[2.5,97.5]).tolist()
            mse_prof_int = np.percentile(mse_prof,[2.5,97.5]).tolist()
            mape_main_int = np.percentile(mape_main,[2.5,97.5]).tolist()
            mape_aux_int = np.percentile(mape_aux,[2.5,97.5]).tolist()
            mape_prof_int = np.percentile(mape_prof,[2.5,97.5]).tolist()
            time_main_int = np.percentile(time_main,[2.5,97.5]).tolist()
            time_aux_int = np.percentile(time_aux,[2.5,97.5]).tolist()
            time_prof_int = np.percentile(time_prof,[2.5,97.5]).tolist()
            search_time_list_int = np.percentile(search_time_list,[2.5,97.5]).tolist()
            mae_main_int_mean = sum(mae_main_int)/2
            mae_aux_int_mean = sum(mae_aux_int)/2
            mae_prof_int_mean = sum(mae_prof_int)/2
            mse_main_int_mean = sum(mse_main_int)/2
            mse_aux_int_mean = sum(mse_aux_int)/2
            mse_prof_int_mean = sum(mse_prof_int)/2
            mape_main_int_mean = sum(mape_main_int)/2
            mape_aux_int_mean = sum(mape_aux_int)/2
            mape_prof_int_mean = sum(mape_prof_int)/2
            time_main_int_mean = sum(time_main_int)/2
            time_aux_int_mean = sum(time_aux_int)/2
            time_prof_int_mean = sum(time_prof_int)/2
            search_time_list_int_mean = sum(search_time_list_int)/2
            mae_main_int_err = (mae_main_int[1] - mae_main_int[0])/2
            mae_aux_int_err = (mae_aux_int[1] - mae_aux_int[0])/2
            mae_prof_int_err = (mae_prof_int[1] - mae_prof_int[0])/2
            mse_main_int_err = (mse_main_int[1] - mse_main_int[0])/2
            mse_aux_int_err = (mse_aux_int[1] - mse_aux_int[0])/2
            mse_prof_int_err = (mse_prof_int[1] - mse_prof_int[0])/2
            mape_main_int_err = (mape_main_int[1] - mape_main_int[0])/2
            mape_aux_int_err = (mape_aux_int[1] - mape_aux_int[0])/2
            mape_prof_int_err = (mape_prof_int[1] - mape_prof_int[0])/2
            time_main_int_err = (time_main_int[1] - time_main_int[0])/2
            time_aux_int_err = (time_aux_int[1] - time_aux_int[0])/2
            time_prof_int_err = (time_prof_int[1] - time_prof_int[0])/2
            search_time_list_int_err = (search_time_list_int[1] - search_time_list_int[0])/2
            data = {
                'mae_main':mae_main,
                'mae_aux':mae_aux,
                'mae_prof':mae_prof,
                'mse_main':mse_main,
                'mse_aux':mse_aux,
                'mse_prof':mse_prof,
                'mape_main':mape_main,
                'mape_aux':mape_aux,
                'mape_prof':mape_prof,
                'time_main':time_main,
                'time_aux':time_aux,
                'time_prof':time_prof,
                'search_time_list':search_time_list,
                'mae_main_int':mae_main_int,
                'mae_aux_int':mae_aux_int,
                'mae_prof_int':mae_prof_int,
                'mse_main_int':mse_main_int,
                'mse_aux_int':mse_aux_int,
                'mse_prof_int':mse_prof_int,
                'mape_main_int':mape_main_int,
                'mape_aux_int':mape_aux_int,
                'mape_prof_int':mape_prof_int,
                'time_main_int':time_main_int,
                'time_aux_int':time_aux_int,
                'time_prof_int':time_prof_int,
                'search_time_list_int':search_time_list_int,
                'mae_main_int_mean':mae_main_int_mean,
                'mae_aux_int_mean':mae_aux_int_mean,
                'mae_prof_int_mean':mae_prof_int_mean,
                'mse_main_int_mean':mse_main_int_mean,
                'mse_aux_int_mean':mse_aux_int_mean,
                'mse_prof_int_mean':mse_prof_int_mean,
                'mape_main_int_mean':mape_main_int_mean,
                'mape_aux_int_mean':mape_aux_int_mean,
                'mape_prof_int_mean':mape_prof_int_mean,
                'time_main_int_mean':time_main_int_mean,
                'time_aux_int_mean':time_aux_int_mean,
                'time_prof_int_mean':time_prof_int_mean,
                'search_time_list_int_mean':search_time_list_int_mean,
                'mae_main_int_err':mae_main_int_err,
                'mae_aux_int_err':mae_aux_int_err,
                'mae_prof_int_err':mae_prof_int_err,
                'mse_main_int_err':mse_main_int_err,
                'mse_aux_int_err':mse_aux_int_err,
                'mse_prof_int_err':mse_prof_int_err,
                'mape_main_int_err':mape_main_int_err,
                'mape_aux_int_err':mape_aux_int_err,
                'mape_prof_int_err':mape_prof_int_err,
                'time_main_int_err':time_main_int_err,
                'time_aux_int_err':time_aux_int_err,
                'time_prof_int_err':time_prof_int_err,
                'search_time_list_int_err':search_time_list_int_err
                }
            with open(work_dir + f'/{model}_{segment_id}.json','w') as f:
                f.write(json.dumps(data))
        if done and mae_prof_int_mean > 0.01 and mse_prof_int_mean > 0.01 and mape_prof_int_mean > 0.01:
            break

models = ['CNN LSTM']
# models = ['Bi-LSTM','LSTM','SVR','CNN','CNN LSTM','GRU','LR']

DATABASE = 'PEMSD8'

work_dir = f'./{DATABASE}'
# path_to_work_dir = f'/content/drive/MyDrive/DATASETS/{DATABASE}-{MODEL}-{HORIZON}'
if not os.path.exists(work_dir):
    os.mkdir(work_dir)

df = pd.read_csv(f'./{DATABASE}.dyna')

df.head()
df.groupby('entity_id').mean().index
df.info()
df.loc[df['entity_id']==716336]['traffic_speed']


get_statistics_for_dataset(df,work_dir=work_dir,verbose=1,bootsrap_iterations=5,custom_segments=[154],epochs=400,studies_column='traffic_flow')

"""# Test al√©atoire"""

def machineLearningTest(df,name:str,epochs,mean_iter=5,min_win=2,max_win=50,verbose=0,batch_size=8,val_split=0.2,patience=8):
    results = []
    times = []
    real,prediction = df.iloc[:,0].to_numpy().reshape(-1,),df.iloc[:,1].to_numpy().reshape(-1,)
    error = real - prediction
    length = len(error)
    test_length = int(length * 0.3)
    
    for window in [2,3,4,5,6,7,8,9,10]:
        x,y = [],[]
        for i in range(len(error) - window):
            x.append(error[i:i+window])
            y.append(error[i+window]) 
        x,y = np.array(x),np.array(y)
        x_train,y_train = x[:test_length],y[:test_length]
        for k in [2,3]:
            knn = KNeighborsRegressor(n_neighbors=k)
            start = ttime.time()
            knn.fit(x_train, y_train)
            end = ttime.time()
            TIME = end - start
            error_prediction = knn.predict(x[test_length:])
            err = mean_absolute_error(real[test_length + window:],prediction[test_length + window:] + error_prediction.reshape(-1,))
            mse_err = mean_squared_error(real[test_length + window:],prediction[test_length + window:] + error_prediction.reshape(-1,))
            mape_err = mape(real[test_length + window:],prediction[test_length + window:] + error_prediction.reshape(-1,))
            result3 = pd.DataFrame(data={'Metrics':['MAE','MSE','MAPE'],'Normal values':[float(mean_absolute_error(real[test_length + window:],prediction[test_length + window:])),float(mean_squared_error(real[test_length + window:],prediction[test_length + window:])),float(mape(real[test_length + window:],prediction[test_length + window:]))],'With Optimization':[float(err),float(mse_err),mape_err],'Profits':[float(mean_absolute_error(real[test_length + window:],prediction[test_length + window:]) - err),float(mean_squared_error(real[test_length + window:],prediction[test_length + window:]) - mse_err),(mape(real[test_length + window:],prediction[test_length + window:]) - mape_err)],'%':[float((mean_absolute_error(real[test_length + window:],prediction[test_length + window:]) - err)/mean_absolute_error(real[test_length + window:],prediction[test_length + window:])),float((mean_squared_error(real[test_length + window:],prediction[test_length + window:]) - mse_err)/mean_squared_error(real[test_length + window:],prediction[test_length + window:])),float((mape(real[test_length + window:],prediction[test_length + window:]) - mape_err)/(mape(real[test_length + window:],prediction[test_length + window:])))]})
            results.append(result3)
            times.append(TIME) 
    best_df = better(results)
    # print(best_df)
    index = 0
    for i in results:
        if all(i != best_df):
            index+=1
            break
        
        
    return better(results),times[index]


        


studied = df[['entity_id','traffic_speed','day','hours']].copy()
entity_ids = pd.DataFrame(df.groupby('entity_id').mean()).index
print('start segments')
done = True
for segment_id in entity_ids:
    print('for segment : ',segment_id)
    model="SVR"
    done = True
    # results_map = {}
    selected_id = studied.loc[studied['entity_id'] == segment_id]
    selected_id.drop(['entity_id'],inplace=True,axis=1)
    x,y = to_supervised(selected_id)
    x_train,y_train,x_test,y_test,x_val,y_val = split(x,y)
    mae_main = []
    mae_aux = []
    mae_prof = []
    mse_main = []
    mse_aux = []
    mse_prof = []
    mape_main = []
    mape_aux = []
    mape_prof = []
    time_main = []
    time_aux = []
    time_prof = []
    search_time_list = []
    y_pred,TIME = train_model(model_name=model,verbose=0,batch_size=256,x_train=x_train,x_test=x_test,y_train=y_train,y_test=y_test,epochs=100,work_dir=work_dir)
    time_main.append(TIME)
    ss=pd.DataFrame({'real':y_test.reshape(-1,),'pred':y_pred.reshape(-1,)})
    # ss.to_csv(work_dir + f'/prediction_{model}_{segment_id}_{int(1000 * random.random())}.csv',index=False)
    start = ttime.time()
    results, TIME_2 = machineLearningTest(df=ss,name='hh',verbose=0,epochs=90)
    end = ttime.time()
    search_time = end - start
    time_aux.append(TIME_2)
    print(f'MAE:{results.iloc[0,4]:2f},MSE:{results.iloc[1,4]:2f},MAPE:{results.iloc[2,4]:2f}')


"""# Error prediction"""

import random
import numpy as np
import pandas as pd
from keras.models import Sequential
from keras.layers import *
from keras.callbacks import Callback
from keras.metrics import *
from keras.callbacks import EarlyStopping
from keras.optimizers import Adam
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler,LabelEncoder
import seaborn as sns
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error,r2_score
from sklearn.svm import SVR
from keras.models import load_model
from sklearn.model_selection import GridSearchCV

def mape(y_test,y_pred):
    c = []
    if len(y_test.shape)==1:
        for i in range(len(y_test)):
            if y_test[i]!=0:
                c.append(abs((y_test[i]-y_pred[i])/y_test[i]))
        return  np.mean(c) * 100
    else:
        d= []
        for i in range(len(y_test[0])):
            d.append(mape(y_test[:,i],y_pred[:,i]))
        return np.mean(d)

df = pd.read_csv('./LSTM_PeMSD7(0).csv')
df.head()

df.info()

y_test,y_pred = df.iloc[:,0].to_numpy().reshape(-1,),df.iloc[:,1].to_numpy().reshape(-1,)

import tensorflow as tf 
def R2_Score(y, prediction): 
    total_error = tf.reduce_sum(tf.square(tf.subtract(y, tf.reduce_mean(y)))) 
    unexplained_error = tf.reduce_sum(tf.square(tf.subtract(y, prediction))) 
    R_squared = tf.subtract(1.0, tf.divide(unexplained_error, total_error)) 
 
    return (R_squared)

print(f"mean_absolute_error = {mean_absolute_error(y_test,y_pred)}")
print(f"mean_absolute_percentage_error = {mape(y_test,y_pred)}")
print(f"mean_squared_error = {mean_squared_error(y_test,y_pred)}")
print(f"R2 error = {r2_score(y_test,y_pred)}")

len(y_test)

x = []
x.append(df.iloc[:3])

np.array(x).shape

def scale_data(error,sc):
  sc = MinMaxScaler()
  error = sc.fit_transform(error.reshape(-1,1)).reshape(-1,)
  return error,sc
def descale_data(error,sc):
  error = sc.inverse_transform(error.reshape(-1,1))
  return error

# def runTests(df_name:str,mean_iter=5,min_win=2,max_win=50,verbose=0,batch_size=8,val_split=0.2,patience=8):
#   print('Starting ...')
#   window = 14
#   early = EarlyStopping(monitor='val_loss',patience=patience)
#   df = pd.read_csv(df_name)
#   print(f'Data has {df.__len__()} item')
#   print('Samples of the data :')
#   print(df.head())
#   real,prediction = df.iloc[:,0].to_numpy().reshape(-1,),df.iloc[:,1].to_numpy().reshape(-1,)
#   error = real - prediction
#   # error = abs(real - prediction)
#   sc = MinMaxScaler()
#   # error = sc.fit_transform(error.reshape(-1,1)).reshape(-1,)
#   print('Samples of the error :')
#   print(error[:5])
#   print(f'Mean error : {np.mean(error):.3f}, Max : {np.max(error):.3f}, Min : {np.min(error):.3f}, Var : {np.var(error):.3f}')
#   length = len(error)
#   test_length = int(length * 0.3)
#   x,y = [],[]
#   for i in range(len(error) - window):
#     x.append(error[i:i+window])
#     y.append(error[i+window]) 
#   x,y = np.array(x),np.array(y)
#   x_train,y_train = x[:test_length],y[:test_length]
#   model = Sequential()
#   model.add((LSTM(25, return_sequences=True, input_shape=(window,1)))) 
#   model.add(BatchNormalization())
#   model.add(Dropout(0.2)) 
#   model.add((LSTM(40,return_sequences=True))) 
#   model.add(BatchNormalization())
#   model.add(Dropout(0.2)) 
#   model.add((LSTM(20, return_sequences=True))) 
#   model.add((LSTM(5))) 
#   model.add(Dense(1))  
#   model.compile(optimizer=Adam(),loss="mae",metrics=[RootMeanSquaredError()])
#   print('Start training ...')
#   history = model.fit(x_train,y_train,epochs=69,callbacks=[early],validation_split=val_split,verbose=verbose,batch_size=batch_size)
#   # for i in range(10,200,10):
#     # print('Training for epochs : ',i)
#   # history = model.fit(x_train,y_train,epochs=10,callbacks=[early],validation_split=val_split,verbose=verbose,batch_size=batch_size)
#   error_prediction = model.predict(x[test_length:])

#   # error_prediction = sc.inverse_transform(error_prediction.reshape(-1,1))

#   # print(f'Mean predicted error : {np.mean(error_prediction):.3f}, Max : {np.max(error_prediction):.3f}, Min : {np.min(error_prediction):.3f}, Var : {np.var(error_prediction):.3f}')
  
#   err = mean_absolute_error(real[test_length + window:],prediction[test_length + window:] + error_prediction.reshape(-1,))
#   mse_err = mean_squared_error(real[test_length + window:],prediction[test_length + window:] + error_prediction.reshape(-1,))
#   nadji_err = r2_score(real[test_length + window:],prediction[test_length + window:] + error_prediction.reshape(-1,))
#   result = pd.DataFrame(data={'Metrics':['MAE','MSE','R2'],'Normal values':[float(mean_absolute_error(real[test_length + window:],prediction[test_length + window:])),float(mean_squared_error(real[test_length + window:],prediction[test_length + window:])),float(r2_score(real[test_length + window:],prediction[test_length + window:]))],'With Optimization':[float(err),float(mse_err),float(nadji_err)],'Profits':[float(mean_absolute_error(real[test_length + window:],prediction[test_length + window:]) - err),float(mean_squared_error(real[test_length + window:],prediction[test_length + window:]) - mse_err),float(nadji_err - r2_score(real[test_length + window:],prediction[test_length + window:]) )],'%':[float((mean_absolute_error(real[test_length + window:],prediction[test_length + window:]) - err)/mean_absolute_error(real[test_length + window:],prediction[test_length + window:])),float((mean_squared_error(real[test_length + window:],prediction[test_length + window:]) - mse_err)/mean_squared_error(real[test_length + window:],prediction[test_length + window:])),float((nadji_err - r2_score(real[test_length + window:],prediction[test_length + window:])/nadji_err ))]})
#   # print(f"MAE :{mean_absolute_error(real[test_length + window:],prediction[test_length + window:]):.3f}, Our MAE : {err:.3f}, Profits : {(mean_absolute_error(real[test_length + window:],prediction[test_length + window:]) - err):.3f}")
#   # print(f"MSE :{mean_squared_error(real[test_length + window:],prediction[test_length + window:]):.3f}, Our MSE : {mse_err:.3f}, Profits : {(mean_squared_error(real[test_length + window:],prediction[test_length + window:]) - mse_err):.3f}")
#   # print(f"R2  : {R2_Score(real[test_length + window:],prediction[test_length + window:]):.3f}, Our R2 : {nadji_err:.3f}, Profits : {(nadji_err - R2_Score(real[test_length + window:],prediction[test_length + window:]) ):.5f}")
#   # print(f"Profits : {(mean_absolute_error(real[test_length + window:],prediction[test_length + window:]) - err):.3f}")
#   print(result.head())
#   print('End training ...')
#   # plt.figure(figsize=(10,6))
#   # ax1 = plt.subplot(211)
#   # ax1.plot(prediction[test_length + window:test_length + window + 100],label='Prediction values')
#   # ax1.plot(real[test_length + window:test_length + window + 100],label='Real values')
#   # ax1.plot(prediction[test_length + window:test_length + window+100] + error_prediction.reshape(-1,),label='Regularized prediction values')
#   # ax1.legend(loc='best')
#   # ax2 = plt.subplot(212)
#   # ax2.plot(history.history['loss'],label='loss')
#   # ax2.plot(history.history['val_loss'],label='val_loss')
#   # ax2.legend(loc='best')
#   # plt.show()

def searchBestPrediction(df_name:str,mean_iter=5,min_win=2,max_win=50,verbose=0,batch_size=8,val_split=0.2,patience=8,window=14):
#   print('Starting ...')
  early = EarlyStopping(monitor='val_loss',patience=patience)
  df = pd.read_csv(df_name)
  real,prediction = df.iloc[:,0].to_numpy().reshape(-1,),df.iloc[:,1].to_numpy().reshape(-1,)
  error = real - prediction
  sc = MinMaxScaler()
  # error = sc.fit_transform(error.reshape(-1,1)).reshape(-1,)
  length = len(error)
  test_length = int(length * 0.3)
  x,y = [],[]
  for i in range(len(error) - window):
    x.append(error[i:i+window])
    y.append(error[i+window]) 
  x,y = np.array(x),np.array(y)
  x_train,y_train = x[:test_length],y[:test_length]
#   model = Sequential()
#   model.add((LSTM(50, return_sequences=True, input_shape=(window,1)))) 
#   model.add(BatchNormalization())
#   model.add(Dropout(0.2)) 
#   model.add((LSTM(40,return_sequences=True))) 
#   model.add(BatchNormalization())
#   model.add(Dropout(0.2)) 
#   model.add((LSTM(20, return_sequences=True))) 
#   model.add((LSTM(5))) 
#   model.add(Dense(1))  
#   model.compile(optimizer=Adam(),loss="mae",metrics=[RootMeanSquaredError()])
#   history = model.fit(x_train,y_train,epochs=69,callbacks=[early],validation_split=val_split,verbose=verbose,batch_size=batch_size)
#   error_prediction = model.predict(x[test_length:])
#   err = mean_absolute_error(real[test_length + window:],prediction[test_length + window:] + error_prediction.reshape(-1,))
#   mse_err = mean_squared_error(real[test_length + window:],prediction[test_length + window:] + error_prediction.reshape(-1,))
#   nadji_err = mape(real[test_length + window:],prediction[test_length + window:] + error_prediction.reshape(-1,))
#   result = pd.DataFrame(data={'Metrics':['MAE','MSE','MAPE'],'Normal values':[float(mean_absolute_error(real[test_length + window:],prediction[test_length + window:])),float(mean_squared_error(real[test_length + window:],prediction[test_length + window:])),float(mape(real[test_length + window:],prediction[test_length + window:]))],'With Optimization':[float(err),float(mse_err),float(nadji_err)],'Profits':[float(mean_absolute_error(real[test_length + window:],prediction[test_length + window:]) - err),float(mean_squared_error(real[test_length + window:],prediction[test_length + window:]) - mse_err),float(mape(real[test_length + window:],prediction[test_length + window:])-nadji_err )],'%':[float((mean_absolute_error(real[test_length + window:],prediction[test_length + window:]) - err)/mean_absolute_error(real[test_length + window:],prediction[test_length + window:])),float((mean_squared_error(real[test_length + window:],prediction[test_length + window:]) - mse_err)/mean_squared_error(real[test_length + window:],prediction[test_length + window:])),float(((mape(real[test_length + window:],prediction[test_length + window:]) - nadji_err) /mape(real[test_length + window:],prediction[test_length + window:]) ))]})
#   # print(f"Profits : {(mean_absolute_error(real[test_length + window:],prediction[test_length + window:]) - err):.3f}")
#   print(f'LSTM => window = {window}, profits : {[float((mean_absolute_error(real[test_length + window:],prediction[test_length + window:]) - err)/mean_absolute_error(real[test_length + window:],prediction[test_length + window:])),float((mean_squared_error(real[test_length + window:],prediction[test_length + window:]) - mse_err)/mean_squared_error(real[test_length + window:],prediction[test_length + window:])),float(((mape(real[test_length + window:],prediction[test_length + window:]) - nadji_err) /mape(real[test_length + window:],prediction[test_length + window:]) ))]}')
#   model = Sequential()
  model.add(Bidirectional(LSTM(64, return_sequences=True), input_shape=(window,1))) 
  model.add(Bidirectional(LSTM(64)))
  model.add(Dense(1))
  model.compile(optimizer=Adam(),loss="mae",metrics=[RootMeanSquaredError()])
  history = model.fit(x_train,y_train,epochs=69,callbacks=[early],validation_split=val_split,verbose=verbose,batch_size=batch_size)
  error_prediction = model.predict(x[test_length:])
  err = mean_absolute_error(real[test_length + window:],prediction[test_length + window:] + error_prediction.reshape(-1,))
  mse_err = mean_squared_error(real[test_length + window:],prediction[test_length + window:] + error_prediction.reshape(-1,))
  nadji_err = mape(real[test_length + window:],prediction[test_length + window:] + error_prediction.reshape(-1,))
  result = pd.DataFrame(data={'Metrics':['MAE','MSE','MAPE'],'Normal values':[float(mean_absolute_error(real[test_length + window:],prediction[test_length + window:])),float(mean_squared_error(real[test_length + window:],prediction[test_length + window:])),float(mape(real[test_length + window:],prediction[test_length + window:]))],'With Optimization':[float(err),float(mse_err),float(nadji_err)],'Profits':[float(mean_absolute_error(real[test_length + window:],prediction[test_length + window:]) - err),float(mean_squared_error(real[test_length + window:],prediction[test_length + window:]) - mse_err),float(mape(real[test_length + window:],prediction[test_length + window:])-nadji_err )],'%':[float((mean_absolute_error(real[test_length + window:],prediction[test_length + window:]) - err)/mean_absolute_error(real[test_length + window:],prediction[test_length + window:])),float((mean_squared_error(real[test_length + window:],prediction[test_length + window:]) - mse_err)/mean_squared_error(real[test_length + window:],prediction[test_length + window:])),float(((mape(real[test_length + window:],prediction[test_length + window:]) - nadji_err) /mape(real[test_length + window:],prediction[test_length + window:]) ))]})
  # print(f"Profits : {(mean_absolute_error(real[test_length + window:],prediction[test_length + window:]) - err):.3f}")
  print(f'Bi-LSTM1 => window = {window}, profits : {[float((mean_absolute_error(real[test_length + window:],prediction[test_length + window:]) - err)/mean_absolute_error(real[test_length + window:],prediction[test_length + window:])),float((mean_squared_error(real[test_length + window:],prediction[test_length + window:]) - mse_err)/mean_squared_error(real[test_length + window:],prediction[test_length + window:])),float(((mape(real[test_length + window:],prediction[test_length + window:]) - nadji_err) /mape(real[test_length + window:],prediction[test_length + window:]) ))]}')
#   model = Sequential()
#   model.add(Bidirectional(LSTM(64, return_sequences=True), input_shape=(window,1))) 
#   model.add(Bidirectional(LSTM(32)))
#   model.add(Dense(1))
#   model.compile(optimizer=Adam(),loss="mae",metrics=[RootMeanSquaredError()])
#   history = model.fit(x_train,y_train,epochs=69,callbacks=[early],validation_split=val_split,verbose=verbose,batch_size=batch_size)
#   error_prediction = model.predict(x[test_length:])
#   err = mean_absolute_error(real[test_length + window:],prediction[test_length + window:] + error_prediction.reshape(-1,))
#   mse_err = mean_squared_error(real[test_length + window:],prediction[test_length + window:] + error_prediction.reshape(-1,))
#   nadji_err = mape(real[test_length + window:],prediction[test_length + window:] + error_prediction.reshape(-1,))
#   result = pd.DataFrame(data={'Metrics':['MAE','MSE','MAPE'],'Normal values':[float(mean_absolute_error(real[test_length + window:],prediction[test_length + window:])),float(mean_squared_error(real[test_length + window:],prediction[test_length + window:])),float(mape(real[test_length + window:],prediction[test_length + window:]))],'With Optimization':[float(err),float(mse_err),float(nadji_err)],'Profits':[float(mean_absolute_error(real[test_length + window:],prediction[test_length + window:]) - err),float(mean_squared_error(real[test_length + window:],prediction[test_length + window:]) - mse_err),float(mape(real[test_length + window:],prediction[test_length + window:])-nadji_err )],'%':[float((mean_absolute_error(real[test_length + window:],prediction[test_length + window:]) - err)/mean_absolute_error(real[test_length + window:],prediction[test_length + window:])),float((mean_squared_error(real[test_length + window:],prediction[test_length + window:]) - mse_err)/mean_squared_error(real[test_length + window:],prediction[test_length + window:])),float(((mape(real[test_length + window:],prediction[test_length + window:]) - nadji_err) /mape(real[test_length + window:],prediction[test_length + window:]) ))]})
#   # print(f"Profits : {(mean_absolute_error(real[test_length + window:],prediction[test_length + window:]) - err):.3f}")
#   print(f'Bi-LSTM2 => window = {window}, profits : {[float((mean_absolute_error(real[test_length + window:],prediction[test_length + window:]) - err)/mean_absolute_error(real[test_length + window:],prediction[test_length + window:])),float((mean_squared_error(real[test_length + window:],prediction[test_length + window:]) - mse_err)/mean_squared_error(real[test_length + window:],prediction[test_length + window:])),float(((mape(real[test_length + window:],prediction[test_length + window:]) - nadji_err) /mape(real[test_length + window:],prediction[test_length + window:]) ))]}')
#   model = Sequential()
#   model.add(Bidirectional(LSTM(128, return_sequences=True), input_shape=(window,1))) 
#   model.add(Bidirectional(LSTM(64, return_sequences=True)))
#   model.add(Bidirectional(LSTM(32)))
#   model.add(Dense(1))
#   model.compile(optimizer=Adam(),loss="mae",metrics=[RootMeanSquaredError()])
#   history = model.fit(x_train,y_train,epochs=69,callbacks=[early],validation_split=val_split,verbose=verbose,batch_size=batch_size)
#   error_prediction = model.predict(x[test_length:])
#   err = mean_absolute_error(real[test_length + window:],prediction[test_length + window:] + error_prediction.reshape(-1,))
#   mse_err = mean_squared_error(real[test_length + window:],prediction[test_length + window:] + error_prediction.reshape(-1,))
#   nadji_err = mape(real[test_length + window:],prediction[test_length + window:] + error_prediction.reshape(-1,))
#   result = pd.DataFrame(data={'Metrics':['MAE','MSE','MAPE'],'Normal values':[float(mean_absolute_error(real[test_length + window:],prediction[test_length + window:])),float(mean_squared_error(real[test_length + window:],prediction[test_length + window:])),float(mape(real[test_length + window:],prediction[test_length + window:]))],'With Optimization':[float(err),float(mse_err),float(nadji_err)],'Profits':[float(mean_absolute_error(real[test_length + window:],prediction[test_length + window:]) - err),float(mean_squared_error(real[test_length + window:],prediction[test_length + window:]) - mse_err),float(mape(real[test_length + window:],prediction[test_length + window:])-nadji_err )],'%':[float((mean_absolute_error(real[test_length + window:],prediction[test_length + window:]) - err)/mean_absolute_error(real[test_length + window:],prediction[test_length + window:])),float((mean_squared_error(real[test_length + window:],prediction[test_length + window:]) - mse_err)/mean_squared_error(real[test_length + window:],prediction[test_length + window:])),float(((mape(real[test_length + window:],prediction[test_length + window:]) - nadji_err) /mape(real[test_length + window:],prediction[test_length + window:]) ))]})
#   # print(f"Profits : {(mean_absolute_error(real[test_length + window:],prediction[test_length + window:]) - err):.3f}")
#   print(f'Bi-LSTM3 => window = {window}, profits : {[float((mean_absolute_error(real[test_length + window:],prediction[test_length + window:]) - err)/mean_absolute_error(real[test_length + window:],prediction[test_length + window:])),float((mean_squared_error(real[test_length + window:],prediction[test_length + window:]) - mse_err)/mean_squared_error(real[test_length + window:],prediction[test_length + window:])),float(((mape(real[test_length + window:],prediction[test_length + window:]) - nadji_err) /mape(real[test_length + window:],prediction[test_length + window:]) ))]}')
#   model = Sequential()
#   model.add(Bidirectional(LSTM(32, return_sequences=True), input_shape=(window,1))) 
#   model.add(Bidirectional(LSTM(16)))
#   model.add(Dense(1))
#   model.compile(optimizer=Adam(),loss="mae",metrics=[RootMeanSquaredError()])
#   history = model.fit(x_train,y_train,epochs=69,callbacks=[early],validation_split=val_split,verbose=verbose,batch_size=batch_size)
#   error_prediction = model.predict(x[test_length:])
#   err = mean_absolute_error(real[test_length + window:],prediction[test_length + window:] + error_prediction.reshape(-1,))
#   mse_err = mean_squared_error(real[test_length + window:],prediction[test_length + window:] + error_prediction.reshape(-1,))
#   nadji_err = mape(real[test_length + window:],prediction[test_length + window:] + error_prediction.reshape(-1,))
#   result = pd.DataFrame(data={'Metrics':['MAE','MSE','MAPE'],'Normal values':[float(mean_absolute_error(real[test_length + window:],prediction[test_length + window:])),float(mean_squared_error(real[test_length + window:],prediction[test_length + window:])),float(mape(real[test_length + window:],prediction[test_length + window:]))],'With Optimization':[float(err),float(mse_err),float(nadji_err)],'Profits':[float(mean_absolute_error(real[test_length + window:],prediction[test_length + window:]) - err),float(mean_squared_error(real[test_length + window:],prediction[test_length + window:]) - mse_err),float(mape(real[test_length + window:],prediction[test_length + window:])-nadji_err )],'%':[float((mean_absolute_error(real[test_length + window:],prediction[test_length + window:]) - err)/mean_absolute_error(real[test_length + window:],prediction[test_length + window:])),float((mean_squared_error(real[test_length + window:],prediction[test_length + window:]) - mse_err)/mean_squared_error(real[test_length + window:],prediction[test_length + window:])),float(((mape(real[test_length + window:],prediction[test_length + window:]) - nadji_err) /mape(real[test_length + window:],prediction[test_length + window:]) ))]})
#   # print(f"Profits : {(mean_absolute_error(real[test_length + window:],prediction[test_length + window:]) - err):.3f}")
#   print(f'Bi-LSTM4 => window = {window}, profits : {[float((mean_absolute_error(real[test_length + window:],prediction[test_length + window:]) - err)/mean_absolute_error(real[test_length + window:],prediction[test_length + window:])),float((mean_squared_error(real[test_length + window:],prediction[test_length + window:]) - mse_err)/mean_squared_error(real[test_length + window:],prediction[test_length + window:])),float(((mape(real[test_length + window:],prediction[test_length + window:]) - nadji_err) /mape(real[test_length + window:],prediction[test_length + window:]) ))]}')
#   model = Sequential()
#   model.add(Bidirectional(LSTM(32, return_sequences=True), input_shape=(window,1))) 
#   model.add(Bidirectional(LSTM(16, return_sequences=True)))
#   model.add(Bidirectional(LSTM(8)))
#   model.add(Dense(1))
#   model.compile(optimizer=Adam(),loss="mae",metrics=[RootMeanSquaredError()])
#   history = model.fit(x_train,y_train,epochs=69,callbacks=[early],validation_split=val_split,verbose=verbose,batch_size=batch_size)
#   error_prediction = model.predict(x[test_length:])
#   err = mean_absolute_error(real[test_length + window:],prediction[test_length + window:] + error_prediction.reshape(-1,))
#   mse_err = mean_squared_error(real[test_length + window:],prediction[test_length + window:] + error_prediction.reshape(-1,))
#   nadji_err = mape(real[test_length + window:],prediction[test_length + window:] + error_prediction.reshape(-1,))
#   result = pd.DataFrame(data={'Metrics':['MAE','MSE','MAPE'],'Normal values':[float(mean_absolute_error(real[test_length + window:],prediction[test_length + window:])),float(mean_squared_error(real[test_length + window:],prediction[test_length + window:])),float(mape(real[test_length + window:],prediction[test_length + window:]))],'With Optimization':[float(err),float(mse_err),float(nadji_err)],'Profits':[float(mean_absolute_error(real[test_length + window:],prediction[test_length + window:]) - err),float(mean_squared_error(real[test_length + window:],prediction[test_length + window:]) - mse_err),float(mape(real[test_length + window:],prediction[test_length + window:])-nadji_err )],'%':[float((mean_absolute_error(real[test_length + window:],prediction[test_length + window:]) - err)/mean_absolute_error(real[test_length + window:],prediction[test_length + window:])),float((mean_squared_error(real[test_length + window:],prediction[test_length + window:]) - mse_err)/mean_squared_error(real[test_length + window:],prediction[test_length + window:])),float(((mape(real[test_length + window:],prediction[test_length + window:]) - nadji_err) /mape(real[test_length + window:],prediction[test_length + window:]) ))]})
#   # print(f"Profits : {(mean_absolute_error(real[test_length + window:],prediction[test_length + window:]) - err):.3f}")
#   print(f'Bi-LSTM5 => window = {window}, profits : {[float((mean_absolute_error(real[test_length + window:],prediction[test_length + window:]) - err)/mean_absolute_error(real[test_length + window:],prediction[test_length + window:])),float((mean_squared_error(real[test_length + window:],prediction[test_length + window:]) - mse_err)/mean_squared_error(real[test_length + window:],prediction[test_length + window:])),float(((mape(real[test_length + window:],prediction[test_length + window:]) - nadji_err) /mape(real[test_length + window:],prediction[test_length + window:]) ))]}')
#   model = Sequential()
#   model.add(Bidirectional(LSTM(128, return_sequences=True), input_shape=(window,1))) 
#   model.add(Bidirectional(LSTM(64, return_sequences=True)))
#   model.add(Bidirectional(LSTM(32, return_sequences=True)))
#   model.add(Bidirectional(LSTM(16, return_sequences=True)))
#   model.add(Bidirectional(LSTM(8)))
#   model.add(Dense(1))
#   model.compile(optimizer=Adam(),loss="mae",metrics=[RootMeanSquaredError()])
#   history = model.fit(x_train,y_train,epochs=69,callbacks=[early],validation_split=val_split,verbose=verbose,batch_size=batch_size)
#   error_prediction = model.predict(x[test_length:])
#   err = mean_absolute_error(real[test_length + window:],prediction[test_length + window:] + error_prediction.reshape(-1,))
#   mse_err = mean_squared_error(real[test_length + window:],prediction[test_length + window:] + error_prediction.reshape(-1,))
#   nadji_err = mape(real[test_length + window:],prediction[test_length + window:] + error_prediction.reshape(-1,))
#   result = pd.DataFrame(data={'Metrics':['MAE','MSE','MAPE'],'Normal values':[float(mean_absolute_error(real[test_length + window:],prediction[test_length + window:])),float(mean_squared_error(real[test_length + window:],prediction[test_length + window:])),float(mape(real[test_length + window:],prediction[test_length + window:]))],'With Optimization':[float(err),float(mse_err),float(nadji_err)],'Profits':[float(mean_absolute_error(real[test_length + window:],prediction[test_length + window:]) - err),float(mean_squared_error(real[test_length + window:],prediction[test_length + window:]) - mse_err),float(mape(real[test_length + window:],prediction[test_length + window:])-nadji_err )],'%':[float((mean_absolute_error(real[test_length + window:],prediction[test_length + window:]) - err)/mean_absolute_error(real[test_length + window:],prediction[test_length + window:])),float((mean_squared_error(real[test_length + window:],prediction[test_length + window:]) - mse_err)/mean_squared_error(real[test_length + window:],prediction[test_length + window:])),float(((mape(real[test_length + window:],prediction[test_length + window:]) - nadji_err) /mape(real[test_length + window:],prediction[test_length + window:]) ))]})
#   # print(f"Profits : {(mean_absolute_error(real[test_length + window:],prediction[test_length + window:]) - err):.3f}")
#   print(f'Bi-LSTM => window = {window}, profits : {[float((mean_absolute_error(real[test_length + window:],prediction[test_length + window:]) - err)/mean_absolute_error(real[test_length + window:],prediction[test_length + window:])),float((mean_squared_error(real[test_length + window:],prediction[test_length + window:]) - mse_err)/mean_squared_error(real[test_length + window:],prediction[test_length + window:])),float(((mape(real[test_length + window:],prediction[test_length + window:]) - nadji_err) /mape(real[test_length + window:],prediction[test_length + window:]) ))]}')

#   print(result.head())
#   print('End training ...')

for i in [2,3,4,5,6,7,8,9,10,11,12,13,14]:
    for _ in range(2):    
        searchBestPrediction('./LSTM_PeMSD7(0).csv',verbose=0,window=i)

def predictionRegularisation(prediction,real,window = 17,test_length = 100):
    early = EarlyStopping(monitor='val_loss',patience=10)
    prediction = prediction.reshape(-1,)
    real  = real.reshape(-1,)
    error = real - prediction
    # plt.plot(error,label='without')
    error = error.reshape(-1,)
    df = pd.DataFrame(data={'error':error,'real':real})
    print(df.head())
    # plt.plot(error,label='normalized')
    sc = MinMaxScaler()
    df = pd.DataFrame(sc.fit_transform(df),columns=['error','real'])
    print(df.head())
    # plt.show()
    x,y= [],[]
    for i in range(len(error) - window):
        x.append(df.iloc[i:i+window])
        y.append(error[i+window]) 
    x,y = np.array(x),np.array(y)
    x_train,y_train = x[:test_length],y[:test_length]
    print(x_train.shape,y_train.shape)
    model = Sequential() 
    # model.add((LSTM(50, activation='relu', return_sequences=True, input_shape=(window,1)))) 
    # model.add(BatchNormalization())
    # model.add(Dropout(0.2)) 
    # model.add(Dropout(0.2)) 
    # model.add((LSTM(50, activation='relu', return_sequences=True))) 
    # model.add(BatchNormalization())
    # model.add((LSTM(20, activation='relu',return_sequences=True))) 
    # model.add(BatchNormalization())
    # model.add(Dropout(0.2)) 
    # model.add((LSTM(15, activation='relu', return_sequences=True))) 
    # model.add((LSTM(5, activation='relu',input_shape=(window,1)))) 
    #-------------------------------
    # model.add(Input(window))
    # model.add(Dense(60))
    # model.add(Dense(100))
    # model.add(Dense(100))
    # model.add(Dense(100))
    # model.add(Dense(100))
    # model.add(Dense(50)) 
    # model.add(Dense(20)) 
    # model.add(Dense(1))
    #-------------------------------
    #-------------------------------
    #-------------------------------
    model.add((LSTM(50, return_sequences=True, input_shape=(window,2)))) 
    model.add(BatchNormalization())
    model.add(Dropout(0.2)) 
    model.add((LSTM(30,return_sequences=True))) 
    model.add(BatchNormalization())
    model.add(Dropout(0.2)) 
    model.add((LSTM(20, return_sequences=True))) 
    model.add((LSTM(5))) 
    model.add(Dense(1)) 
    #-------------------------------
    # model.add(Dense(window)) 
    # model.add(Dense(60))
    # # model.add(Dense(100))
    # model.add(Dense(50)) 
    # model.add(Dense(20)) 
    # model.add(Dense(1)) 
    #----------------------------
    # model.add((LSTM(50, return_sequences=True, input_shape=(window,1)))) 
    # model.add((LSTM(20, return_sequences=True))) 
    # model.add((LSTM(100, return_sequences=True))) 
    # model.add((LSTM(80, return_sequences=True))) 
    # model.add((LSTM(70, return_sequences=True))) 
    # model.add((LSTM(60, return_sequences=True))) 
    # model.add((LSTM(50, return_sequences=True))) 
    # model.add((LSTM(40, return_sequences=True))) 
    # model.add((LSTM(30, return_sequences=True))) 
    # model.add((LSTM(20, return_sequences=True))) 
    # model.add((LSTM(10))) 
    # model.add(Dense(1)) 
    #-------------------------------
    # model.add(Dense(window)) 
    # model.add(Dense(layers[0]))
    # model.add(Dense(layers[2])) 
    # model.add(Dense(1)) 
    #----------------------------
    model.compile(optimizer=Adam(),loss="mae",metrics=[RootMeanSquaredError()])
    history = model.fit(x_train,y_train,epochs=150,callbacks=[early],validation_split=0.1,verbose=1,batch_size=2)
    error_prediction = model.predict(x[test_length:])
    print()
    df = pd.DataFrame(data={'error':error_prediction,'real':error_prediction})
    print(df.head())
    error_prediction = sc.inverse_transform(df)
    error_prediction = df['error']
    err = mean_absolute_error(real[test_length + window:],prediction[test_length + window:] + error_prediction.reshape(-1,))
    mse_err = mean_squared_error(real[test_length + window:],prediction[test_length + window:] + error_prediction.reshape(-1,))
    nadji_err = R2_Score(real[test_length + window:],prediction[test_length + window:] + error_prediction.reshape(-1,))
    # model.save(f'/lastTest/Error_{err}_window_{window}.h5')

    # svr = SVR(epsilon=0.3)
    # svr.fit(x_train.reshape(-1,window),y_train.ravel())
    # error_prediction = svr.predict(x[test_length:].reshape(-1,window))
    print(f"The original error MAE : {mean_absolute_error(real[test_length + window:],prediction[test_length + window:])}")
    print(f"The regularized error MAE : {err}")
    print(f"The original error MSE : {mean_squared_error(real[test_length + window:],prediction[test_length + window:])}")
    print(f"The regularized error MSE : {mse_err}")
    print(f"The original Nadji error : {R2_Score(real[test_length + window:],prediction[test_length + window:])}")
    print(f"The regularized Nadji error  : {nadji_err}")
    # plt.figure(figsize=(12,8))
    # plt.plot(prediction[test_length + window:],label='Prediction values')
    # plt.plot(real[test_length + window:],label='Real values')
    # plt.plot(prediction[test_length + window:] + error_prediction.reshape(-1,),label='Regularized prediction values')
    # plt.text(2, 14, f"The original error MAE : {mean_absolute_error(real[test_length + window:],prediction[test_length + window:])}", fontsize=10, color='red')
    # plt.text(2, 12, f"The regularized error MAE : {mean_absolute_error(real[test_length + window:],prediction[test_length + window:] - error_prediction.reshape(-1,))}", fontsize=10, color='red')
    # plt.legend(loc='best')
    # plt.show()

def predictionRegularisation(prediction,real,window = 17,test_length = 100):
    early = EarlyStopping(monitor='val_loss',patience=10)
    prediction = prediction.reshape(-1,)
    real  = real.reshape(-1,)
    error = real - prediction
    # plt.plot(error,label='without')
    error = error.reshape(-1,)
    # df = pd.DataFrame(data={'error':error,'real':real})
    # plt.plot(error,label='normalized')
    # sc = MinMaxScaler()
    # df = pd.DataFrame(sc.fit_transform(df),columns=['error','real'])
    # plt.show()
    x,y= [],[]
    for i in range(len(error) - window):
        x.append(error[i:i+window])
        y.append(error[i+window]) 
    x,y = np.array(x),np.array(y)
    x_train,y_train = x[:test_length],y[:test_length]
    # print(x_train.shape,y_train.shape)
    model = Sequential() 
    # model.add((LSTM(50, activation='relu', return_sequences=True, input_shape=(window,1)))) 
    # model.add(BatchNormalization())
    # model.add(Dropout(0.2)) 
    # model.add(Dropout(0.2)) 
    # model.add((LSTM(50, activation='relu', return_sequences=True))) 
    # model.add(BatchNormalization())
    # model.add((LSTM(20, activation='relu',return_sequences=True))) 
    # model.add(BatchNormalization())
    # model.add(Dropout(0.2)) 
    # model.add((LSTM(15, activation='relu', return_sequences=True))) 
    # model.add((LSTM(5, activation='relu',input_shape=(window,1)))) 
    #-------------------------------
    # model.add(Input(window))
    # model.add(Dense(60))
    # model.add(Dense(100))
    # model.add(Dense(100))
    # model.add(Dense(100))
    # model.add(Dense(100))
    # model.add(Dense(50)) 
    # model.add(Dense(20)) 
    # model.add(Dense(1))
    #-------------------------------
    #-------------------------------
    #-------------------------------
    model.add((LSTM(50, return_sequences=True, input_shape=(window,1)))) 
    model.add(BatchNormalization())
    model.add(Dropout(0.2)) 
    model.add((LSTM(30,return_sequences=True))) 
    model.add(BatchNormalization())
    model.add(Dropout(0.2)) 
    model.add((LSTM(20, return_sequences=True))) 
    model.add((LSTM(5))) 
    model.add(Dense(1)) 
    #-------------------------------
    # model.add(Dense(window)) 
    # model.add(Dense(60))
    # # model.add(Dense(100))
    # model.add(Dense(50)) 
    # model.add(Dense(20)) 
    # model.add(Dense(1)) 
    #----------------------------
    # model.add((LSTM(50, return_sequences=True, input_shape=(window,1)))) 
    # model.add((LSTM(20, return_sequences=True))) 
    # model.add((LSTM(100, return_sequences=True))) 
    # model.add((LSTM(80, return_sequences=True))) 
    # model.add((LSTM(70, return_sequences=True))) 
    # model.add((LSTM(60, return_sequences=True))) 
    # model.add((LSTM(50, return_sequences=True))) 
    # model.add((LSTM(40, return_sequences=True))) 
    # model.add((LSTM(30, return_sequences=True))) 
    # model.add((LSTM(20, return_sequences=True))) 
    # model.add((LSTM(10))) 
    # model.add(Dense(1)) 
    #-------------------------------
    # model.add(Dense(window)) 
    # model.add(Dense(layers[0]))
    # model.add(Dense(layers[2])) 
    # model.add(Dense(1)) 
    #----------------------------
    model.compile(optimizer=Adam(),loss="mae",metrics=[RootMeanSquaredError()])
    history = model.fit(x_train,y_train,epochs=150,callbacks=[early],validation_split=0.1,verbose=0,batch_size=2)
    error_prediction = model.predict(x[test_length:])
    # df = pd.DataFrame(data={'error':error_prediction,'real':error_prediction})
    # print(df.head())
    # error_prediction = sc.inverse_transform(df)
    # error_prediction = df['error']
    err = mean_absolute_error(real[test_length + window:],prediction[test_length + window:] + error_prediction.reshape(-1,))
    mse_err = mean_squared_error(real[test_length + window:],prediction[test_length + window:] + error_prediction.reshape(-1,))
    nadji_err = R2_Score(real[test_length + window:],prediction[test_length + window:] + error_prediction.reshape(-1,))
    # model.save(f'/lastTest/Error_{err}_window_{window}.h5')

    # svr = SVR(epsilon=0.3)
    # svr.fit(x_train.reshape(-1,window),y_train.ravel())
    # error_prediction = svr.predict(x[test_length:].reshape(-1,window))
    print(f"MAE : {mean_absolute_error(real[test_length + window:],prediction[test_length + window:]):.3f}, Our MAE : {err:.3f}, Profits : {(mean_absolute_error(real[test_length + window:],prediction[test_length + window:]) - err):.3f}")
    print(f"MSE : {mean_squared_error(real[test_length + window:],prediction[test_length + window:]):.3f}, Our MSE : {mse_err:.3f}, Profits : {(mean_squared_error(real[test_length + window:],prediction[test_length + window:]) - mse_err):.3f}")
    print(f"R2 : {R2_Score(real[test_length + window:],prediction[test_length + window:]):.3f}, Our R2 : {nadji_err:.3f}, Profits : {(nadji_err - R2_Score(real[test_length + window:],prediction[test_length + window:]) ):.5f}")
    # plt.figure(figsize=(12,8))
    # plt.plot(prediction[test_length + window:],label='Prediction values')
    # plt.plot(real[test_length + window:],label='Real values')
    # plt.plot(prediction[test_length + window:] + error_prediction.reshape(-1,),label='Regularized prediction values')
    # plt.text(2, 14, f"The original error MAE : {mean_absolute_error(real[test_length + window:],prediction[test_length + window:])}", fontsize=10, color='red')
    # plt.text(2, 12, f"The regularized error MAE : {mean_absolute_error(real[test_length + window:],prediction[test_length + window:] - error_prediction.reshape(-1,))}", fontsize=10, color='red')
    # plt.legend(loc='best')
    # plt.show()

predictionRegularisation(y_pred,real=y_test,window=13)

for i in range(2,20):
  print(f'for i = {i}')
  predictionRegularisation(y_pred,real=y_test,window=i)

for i in [10,20,30,40,50]:
  for j in [10,20,30,40,50]:
    for k in [10,20,30,40,50]:
      for l in [10,20,30,40,50]:
        print(f'for i={i},j={j},k={k},l={l}')
        predictionRegularisation(y_pred,real=y_test,layers=[i,j,k,l],window=13)

real = y_test
prediction = y_pred
window = 171
test_length=5000
error = real - prediction
prediction = prediction.reshape(-1,)
real  = real.reshape(-1,)
error = real - prediction
x,y= [],[]
for i in range(len(error) - window):
    x.append(error[i:i+window])
    y.append(error[i+window]) 
x,y = np.array(x),np.array(y)
x_train,y_train = x[:test_length],y[:test_length]
model = load_model('/content/saved_model_after_171_0.8120753319241244.h5')
error_prediction = model.predict(x[test_length:].reshape(-1,window))
err = mean_absolute_error(real[test_length + window:],prediction[test_length + window:] + error_prediction.reshape(-1,))
# model.save(f'/lastTest/Window_{window}_error_{err}.h5')

# svr = SVR(epsilon=0.3)
# svr.fit(x_train.reshape(-1,window),y_train.ravel())
# error_prediction = svr.predict(x[test_length:].reshape(-1,window))
print(f"The original error MAE : {mean_absolute_error(real[test_length + window:],prediction[test_length + window:])}")
print(f"The regularized error MAE : {err}")









y_test

real = y_test
prediction = y_pred
window = 181
test_length=5000
error = real - prediction
x,y= [],[]
for i in range(len(error) - window):
    x.append(error[i:i+window])
    y.append(error[i+window]) 
x,y = np.array(x),np.array(y)
x_train,y_train = x[:test_length],y[:test_length]
grid = GridSearchCV(estimator=SVR(),cv=2,verbose = 3,param_grid={"kernel":["rbf","linear","poly", 'sigmoid'],
                                                "epsilon":[0.1,0.05,0.02,0.07,0.2,0.3]},scoring="neg_mean_absolute_error")
grid.fit(x_train,y_train)

print(grid.best_params_)

def predictionRegularisation(prediction,real,window = 17,test_length = 5000):
    """sumary_line
    
    Keyword arguments:
    argument -- description
    Return: return_description
    """  
    early = EarlyStopping(monitor='val_loss',patience=8)

    prediction = prediction.reshape(-1,)
    real  = real.reshape(-1,)
    error = real - prediction
    x,y= [],[]
    for i in range(len(error) - window):
        x.append(error[i:i+window])
        y.append(error[i+window]) 
    x,y = np.array(x),np.array(y)
    x_train,y_train = x[:test_length],y[:test_length]
    model = Sequential() 
    model.add(Input(window)) 
    model.add(Dense(50))
    model.add(Dense(100))
    model.add(Dense(20)) 
    model.add(Dense(1)) 
    model.compile(optimizer=Adam(),loss="mae",metrics=[RootMeanSquaredError()])
    rand = random.random()
    history = model.fit(x_train,y_train,epochs=50,callbacks=[early],validation_split=0.2,verbose=0,batch_size=4)
    error_prediction = model.predict(x[test_length:].reshape(-1,window))
    err = mean_absolute_error(real[test_length + window:],prediction[test_length + window:] - error_prediction.reshape(-1,))
    model.save(f'saved_model_after_{window}_err={err}.h5')
    # svr = SVR(epsilon=0.3)
    # svr.fit(x_train.reshape(-1,window),y_train.ravel())
    # error_prediction = svr.predict(x[test_length:].reshape(-1,window))
    print(f"The original error MAE : {mean_absolute_error(real[test_length + window:],prediction[test_length + window:])}")
    print(f"The regularized error MAE : {err}")

for i in range(170,182):
    print(f'for i = ')
    predictionRegularisation(y_pred,real=y_test,window=180)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from keras.layers import *
from keras.losses import MeanSquaredError
from keras.optimizers import Adam
from keras.metrics import RootMeanSquaredError
from keras.callbacks import ModelCheckpoint, EarlyStopping
# from sklearn.svm import SVR
# from sklearn.neural_network import MLPRegressor
from keras.models import Sequential
# from sklearn.preprocessing import StandardScaler, LabelEncoder
# from sklearn.model_selection import train_test_split
# from statsmodels.tsa.arima.model import ARIMA
# import seaborn as sns
# from math import sqrt
# from matplotlib import pyplot
# from statsmodels.tsa.arima.model import ARIMA
# from sklearn.metrics import *

df = pd.read_csv('./normalized.csv')



df = df[['speed']]
x,y = [],[]
for i in range(df.__len__() - 5):
    x.append(df.iloc[i:i+5])
    y.append(df.iloc[i + 5])

x_train = np.array(x)
y_train = np.array(y)

early = EarlyStopping(monitor='val_loss',patience=3)

x_train,y_train,x_val,y_val = x_train[:500],y_train[:500],x_train[500:],y_train[500:]

model = Sequential() 
# model.add(Conv1D(filters=256, kernel_size=8, padding='same', activation='relu', input_shape=(5, 1))) 
# model.add(MaxPooling1D(pool_size=2, padding='same')) 
# model.add((LSTM(250, activation='relu', return_sequences=True))) 
# model.add((LSTM(200, activation='relu', return_sequences=True))) 
model.add((LSTM(100, activation='relu',return_sequences=True))) 
model.add((LSTM(50, activation='relu'))) 
model.add(Dense(1, activation='relu')) 
model.compile(optimizer=Adam(learning_rate=0.0001),loss="mae",metrics=['mse'])
history = model.fit(x_train,y_train,epochs=50,verbose=1,validation_data=(x_val,y_val) ,batch_size=16,callbacks=[early])

model.save('best_model_cnn_lstm0.h5')
plt.figure(figsize=(15,4))
ax = plt.subplot(121)
ax.plot(history.history['loss'],label='loss')
ax.plot(history.history['val_loss'],label='validation')
ax.legend(loc='best')
ax2 = plt.subplot(122)
ax2.plot(y_val[:100],label='real')
ax2.plot(model.predict(x_val[:100]),label='prediction')
ax2.legend(loc='best')
plt.savefig('result0.png')
plt.show()

early = EarlyStopping(monitor='val_loss',patience=5)

def train():
    for i in range(20):
        model = Sequential() 
        model.add(Conv1D(filters=256, kernel_size=8, padding='same', activation='relu', input_shape=(10, 1))) 
        model.add(MaxPooling1D(pool_size=2, padding='same')) 
        model.add(Flatten()) 
        model.add(RepeatVector(1)) 
        model.add((LSTM(250, activation='relu', return_sequences=True, input_shape=(10,1)))) 
        # model.add(Dropout(0.2)) 
        model.add((LSTM(200, activation='relu', return_sequences=True))) 
        model.add((LSTM(100, activation='relu',return_sequences=True))) 
        # model.add(Dropout(0.5)) 
        model.add((LSTM(50, activation='relu'))) 
        model.add(Dense(1, activation='relu')) 
        # model.add(Dense(n_outputs)) 
        model.compile(optimizer="adam",loss="mae",metrics=['mape'])
        history = model.fit(x_train,y_train,epochs=50,verbose=1,validation_data=(x_val,y_val) ,batch_size=16)

        model.save('best_model_cnn_lstm_N_{}.h5'.format(i))
        plt.figure(figsize=(10,4))
        ax = plt.subplot(121)
        ax.plot(history.history['loss'],label='loss')
        ax.plot(history.history['val_loss'],label='validation')
        ax.legend(loc='best')
        ax2 = plt.subplot(122)
        ax2.plot(y_val[:100],label='real')
        ax2.plot(model.predict(x_val[:100]),label='prediction')
        ax2.legend(loc='best')
        plt.savefig('result_N_{}.png'.format(i))

x,y = [],[]
for i in range(df.__len__() - 15):
    x.append(df.iloc[i:i+15])
    y.append(df.iloc[i + 15])
x_train = np.array(x)
y_train = np.array(y)
# x_train,y_train,x_test,y_test,x_val,y_val = x_train[:18000],y_train[:18000],x_train[18000:22000],y_train[18000:22000],x_train[22000:],y_train[22000:]

df[45:80].plot()

n_timesteps = 15
n_features =1
n_outputs = 1
model = Sequential()
model.add(Conv1D(filters=256, kernel_size=5, padding='same', activation='relu', input_shape=(n_timesteps, n_features)))
model.add(MaxPooling1D(pool_size=2, padding='same'))
model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu', input_shape=(n_timesteps, n_features)))
model.add(MaxPooling1D(pool_size=2, padding='same',))
model.add(Conv1D(filters=4, kernel_size=1, padding='same', activation='relu', input_shape=(n_timesteps, n_features)))
model.add(MaxPooling1D(pool_size=2, padding='same'))
model.add(Flatten())
model.add(RepeatVector(n_outputs))
model.add((LSTM(200, activation='relu', return_sequences=True, input_shape=(n_timesteps, n_features))))
model.add(Dropout(0))
model.add((LSTM(200, activation='relu', return_sequences=True, input_shape=(n_timesteps, n_features))))
model.add(Dropout(0))
model.add((LSTM(200, activation='relu', return_sequences=True, input_shape=(n_timesteps, n_features))))
model.add(Dropout(0))
model.add((LSTM(200, activation='relu', input_shape=(n_timesteps, n_features))))
model.add(Dense(200, activation='relu'))
model.add(Dense(n_outputs))
model.compile(loss='mse', optimizer='adam')

model_name='15_times_1'
history = model.fit(x_train,y_train,epochs=50,verbose=1,validation_data=(x_val,y_val) ,batch_size=64,callbacks=[early])
print(model.evaluate(x_test,y_test))
model.save(f'best_model_cnn_lstm_{model_name}.h5')
plt.figure(figsize=(10,4))
ax = plt.subplot(121)
ax.plot(history.history['loss'],label='loss')
ax.plot(history.history['val_loss'],label='validation')
ax.legend(loc='best')
ax2 = plt.subplot(122)
ax2.plot(y_val[:500],label='real')
ax2.plot(model.predict(x_val[:500]),label='prediction')
ax2.legend(loc='best')
plt.savefig(f'result_{model_name}.png')

def train_model(n_samples:int):
  x,y = [],[]
  for i in range(df.__len__() - n_samples):
      x.append(df.iloc[i:i+n_samples])
      y.append(df.iloc[i + n_samples])

  x_train = np.array(x)
  y_train = np.array(y)

  x_train,y_train,x_val,y_val = x_train[:20000],y_train[:20000],x_train[20000:],y_train[20000:]

  model = Sequential() 
  model.add(Conv1D(filters=256, kernel_size=8, padding='same', activation='relu', input_shape=(n_samples, 1))) 
  model.add(MaxPooling1D(pool_size=2, padding='same')) 
  model.add((LSTM(250, activation='relu', return_sequences=True))) 
  model.add((LSTM(200, activation='relu', return_sequences=True))) 
  model.add((LSTM(100, activation='relu',return_sequences=True))) 
  model.add((LSTM(50, activation='relu'))) 
  model.add(Dense(1, activation='relu')) 
  model.compile(optimizer="adam",loss="mae",metrics=['mape'])
  history = model.fit(x_train,y_train,epochs=20,verbose=1,validation_data=(x_val,y_val) ,batch_size=16,callbacks=[early])
  model.save('best_model_cnn_lstm_n_samples_{}.h5'.format(n_samples))
  plt.figure(figsize=(10,4))
  ax = plt.subplot(121)
  ax.plot(history.history['loss'],label='loss')
  ax.plot(history.history['val_loss'],label='validation')
  ax.legend(loc='best')
  ax2 = plt.subplot(122)
  ax2.plot(y_val[:100],label='real')
  ax2.plot(model.predict(x_val[:100]),label='prediction')
  ax2.legend(loc='best')
  plt.savefig('result_n_samples_{}.png'.format(n_samples))

model2 = Sequential()
model2.add((LSTM(64, activation='relu', input_shape=(5,2))))
model2.add(Dense(25, activation='relu'))
model2.add(Dense(1, activation='linear'))
model2.compile(optimizer=Adam(learning_rate=0.001),loss="mae",metrics=[RootMeanSquaredError()])

WINDOW_SIZE = 10
def to_supervised(df, window_size=5):
  x = []
  y = []
  for i in range(0,len(df) - window_size):
    row = [a for a in df.iloc[i:i+ window_size,:]]
    label = df.iloc[i+ window_size,-1]
    x.append(row)
    y.append(label)
  return np.array(x),np.array(y)
